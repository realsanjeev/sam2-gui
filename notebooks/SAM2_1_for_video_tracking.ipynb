{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDJKe3XNBHeA"
      },
      "source": [
        "### SAM2.1 for Video Tracking\n",
        "\n",
        "**References**\n",
        "- https://github.com/facebookresearch/sam2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dayGiZaDbvvq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57081673-ab8b-4bd5-fd79-da87aaf0a092"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'sam2'...\n",
            "remote: Enumerating objects: 646, done.\u001b[K\n",
            "remote: Counting objects: 100% (646/646), done.\u001b[K\n",
            "remote: Compressing objects: 100% (500/500), done.\u001b[K\n",
            "remote: Total 646 (delta 126), reused 625 (delta 124), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (646/646), 53.14 MiB | 14.85 MiB/s, done.\n",
            "Resolving deltas: 100% (126/126), done.\n",
            "/content/sam2\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building editable for SAM-2 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!git clone --depth=1 https://github.com/facebookresearch/sam2.git\n",
        "%cd sam2\n",
        "%pip install -e . -q\n",
        "%pip install gradio -q\n",
        "%pip install loguru -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8QnHCvW140ez"
      },
      "outputs": [],
      "source": [
        "# downoad the checkpoint\n",
        "%%bash\n",
        "cd checkpoints && \\\n",
        "./download_ckpts.sh > /dev/null 2>&1 && \\\n",
        "cd .."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6hCZLiSBfyA"
      },
      "source": [
        "### Mounting the drive for providing the video\n",
        "> You may also upload in the session. But it is too tedious to upload in the google colab. Oh, well, you may want to use local machine to run this, but make sure to donot track many frame at once, it willl OOM.\n",
        "\n",
        "In my 4GB GPU, it can only process 30 frame(1 sec video) to track.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8514642",
        "outputId": "9464b825-f923-4dbe-8088-bf40c9d55bef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mouting drive so I can directly copy a video file from drive\n",
        "# instead of uploading file in every runtime\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "BcQdc8pHbk-a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e23bf12f-bdd4-4848-ca9f-ddbed388dccd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on device: cuda\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import subprocess\n",
        "from datetime import timedelta\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple, Optional\n",
        "\n",
        "import cv2\n",
        "import gradio as gr\n",
        "import imageio.v2 as iio\n",
        "import numpy as np\n",
        "from loguru import logger as guru\n",
        "\n",
        "import torch\n",
        "import colorsys\n",
        "from sam2.build_sam import build_sam2_video_predictor\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# Device & Precision Configuration\n",
        "# ------------------------------\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Running on device: {device}\")\n",
        "\n",
        "# Use bfloat16 autocast for mixed precision\n",
        "torch.autocast(device_type=device, dtype=torch.bfloat16).__enter__()\n",
        "\n",
        "if device == \"cuda\":\n",
        "    props = torch.cuda.get_device_properties(0)\n",
        "\n",
        "    # Enable TensorFloat-32 (TF32) on Ampere+ GPUs for performance\n",
        "    if props.major >= 8:\n",
        "        torch.backends.cuda.matmul.allow_tf32 = True\n",
        "        torch.backends.cudnn.allow_tf32 = True\n",
        "        torch.backends.cuda.matmul.fp32_precision = \"ieee\"\n",
        "        torch.backends.cudnn.fp32_precision = \"ieee\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "lFqp7GXpbHFo"
      },
      "outputs": [],
      "source": [
        "class PromptGUI:\n",
        "    def __init__(self, checkpoint_dir: str, model_cfg: str, device: str=\"cuda\"):\n",
        "        self.checkpoint_dir = checkpoint_dir\n",
        "        self.model_cfg = model_cfg\n",
        "        self.device = device\n",
        "\n",
        "        # Core state\n",
        "        self.sam_model = None\n",
        "        self.inference_state = None\n",
        "\n",
        "        self.tracker = None\n",
        "\n",
        "        self.selected_points = []\n",
        "        self.selected_labels = []\n",
        "        self.cur_label_val = 1.0\n",
        "\n",
        "        self.frame_index = 0\n",
        "        self.image = None\n",
        "        self.cur_mask_idx = 0\n",
        "        # can store multiple object masks\n",
        "        # saves the masks and logits for each mask index\n",
        "        self.cur_masks = {}\n",
        "        self.cur_logits = {}\n",
        "        self.index_masks_all = []\n",
        "        self.color_masks_all = []\n",
        "\n",
        "        # Image directory\n",
        "        self.img_dir: str = \"\"\n",
        "        self.img_paths: list[str] = []\n",
        "        self._init_sam_model()\n",
        "\n",
        "    # -------------------------------------------------------------\n",
        "    # Initialization & reset\n",
        "    # -------------------------------------------------------------\n",
        "    def _init_sam_model(self) -> None:\n",
        "        \"\"\"Load the SAM model if it hasn't been initialized yet.\"\"\"\n",
        "        if self.sam_model is None:\n",
        "            self.sam_model = build_sam2_video_predictor(self.model_cfg, self.checkpoint_dir, device=device)\n",
        "            guru.info(f\"Loaded model checkpoint from {self.checkpoint_dir}\")\n",
        "\n",
        "    def _clear_image(self) -> None:\n",
        "        \"\"\"Reset current image and all related mask/logit data.\"\"\"\n",
        "        self.image = None\n",
        "        self.cur_mask_idx = 0\n",
        "        self.frame_index = 0\n",
        "        self.cur_masks.clear()\n",
        "        self.cur_logits.clear()\n",
        "        self.index_masks_all.clear()\n",
        "        self.color_masks_all.clear()\n",
        "\n",
        "    def reset(self) -> None:\n",
        "        \"\"\"Completely reset image data and inference state.\"\"\"\n",
        "        self._clear_image()\n",
        "        if self.inference_state is not None:\n",
        "            self.sam_model.reset_state(self.inference_state)\n",
        "\n",
        "    # -------------------------------------------------------------\n",
        "    # Image management\n",
        "    # -------------------------------------------------------------\n",
        "    def set_img_dir(self, img_dir: str) -> int:\n",
        "        \"\"\"Load and store all image paths from the given directory.\"\"\"\n",
        "        self._clear_image()\n",
        "        self.img_dir = img_dir\n",
        "        self.img_paths = [\n",
        "            os.path.join(img_dir, f) for f in sorted(os.listdir(img_dir)) if is_image(f)\n",
        "        ]\n",
        "\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    def set_input_image(self, index: int = 0) -> Optional[np.ndarray]:\n",
        "        \"\"\"Set the current image by index from loaded paths.\"\"\"\n",
        "        guru.debug(f\"Setting frame {index} / {len(self.img_paths)}\")\n",
        "        if index < 0 or index >= len(self.img_paths):\n",
        "            return self.image\n",
        "        self.clear_points()\n",
        "        self.frame_index = index\n",
        "        self.image = iio.imread(self.img_paths[index])\n",
        "        return self.image\n",
        "\n",
        "    # -------------------------------------------------------------\n",
        "    # Point and mask management\n",
        "    # -------------------------------------------------------------\n",
        "    def clear_points(self) -> Tuple[None, None, str]:\n",
        "        \"\"\"Clear all selected points and labels.\"\"\"\n",
        "        self.selected_points.clear()\n",
        "        self.selected_labels.clear()\n",
        "        message = \"Cleared points, select new points to update mask\"\n",
        "        return None, None, message\n",
        "\n",
        "    def add_new_mask(self) -> Tuple[None, str]:\n",
        "        self.cur_mask_idx += 1\n",
        "        self.clear_points()\n",
        "        message = f\"Creating new mask with index {self.cur_mask_idx}\"\n",
        "        return None, message\n",
        "\n",
        "    def make_index_mask(self, masks):\n",
        "        assert len(masks) > 0\n",
        "        idcs = list(masks.keys())\n",
        "        idx_mask = masks[idcs[0]].astype(\"uint8\")\n",
        "        for i in idcs:\n",
        "            mask = masks[i]\n",
        "            idx_mask[mask] = i + 1\n",
        "        return idx_mask\n",
        "\n",
        "    # -------------------------------------------------------------\n",
        "    # SAM Model Interaction\n",
        "    # -------------------------------------------------------------\n",
        "    def get_sam_features(self) -> Tuple[str, np.ndarray | None]:\n",
        "        self.inference_state = self.sam_model.init_state(video_path=self.img_dir)\n",
        "        self.sam_model.reset_state(self.inference_state)\n",
        "        msg = (\n",
        "            \"SAM features extracted. Click points to update mask, \"\n",
        "            \"and submit when ready to start tracking.\"\n",
        "        )\n",
        "        return msg, self.image\n",
        "\n",
        "    def set_positive(self) -> str:\n",
        "        \"\"\"Set label for next points as positive.\"\"\"\n",
        "        self.cur_label_val = 1.0\n",
        "        return \"Selecting positive points. Submit the mask to start tracking.\"\n",
        "\n",
        "    def set_negative(self) -> str:\n",
        "        \"\"\"Set label for next points as negative.\"\"\"\n",
        "        self.cur_label_val = 0.0\n",
        "        return \"Selecting negative points. Submit the mask to start tracking.\"\n",
        "\n",
        "    def add_point(self, frame_idx: int, i: int, j: int) -> np.ndarray:\n",
        "        \"\"\"Add a user-selected point and update corresponding mask.\"\"\"\n",
        "        self.selected_points.append([j, i])\n",
        "        self.selected_labels.append(self.cur_label_val)\n",
        "        # masks, scores, logits if we want to update the mask\n",
        "        masks = self.get_sam_mask(\n",
        "            frame_idx,\n",
        "            np.array(self.selected_points, dtype=np.float32),\n",
        "            np.array(self.selected_labels, dtype=np.int32),\n",
        "        )\n",
        "        mask = self.make_index_mask(masks)\n",
        "\n",
        "        return mask\n",
        "\n",
        "\n",
        "    def get_sam_mask(self, frame_idx: int, input_points: np.ndarray, input_labels: np.ndarray):\n",
        "        \"\"\"Get the SAM mask based on the selected points and labels.\"\"\"\n",
        "        assert self.sam_model is not None, \"SAM model not initialized.\"\n",
        "\n",
        "        with torch.autocast(device_type=self.device, dtype=torch.bfloat16):\n",
        "            _, obj_ids, mask_logits = self.sam_model.add_new_points_or_box(\n",
        "                inference_state=self.inference_state,\n",
        "                frame_idx=frame_idx,\n",
        "                obj_id=self.cur_mask_idx,\n",
        "                points=input_points,\n",
        "                labels=input_labels,\n",
        "            )\n",
        "\n",
        "        return {\n",
        "                obj_id: (mask_logits[i] > 0.0).squeeze().cpu().numpy()\n",
        "                for i, obj_id in enumerate(obj_ids)\n",
        "            }\n",
        "\n",
        "    def run_tracker(self) -> Tuple[str, str]:\n",
        "\n",
        "        # read images and drop the alpha channel\n",
        "        images = [iio.imread(p)[:, :, :3] for p in self.img_paths]\n",
        "\n",
        "        video_segments = {}  # video_segments contains the per-frame segmentation results\n",
        "\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
        "            for out_frame_idx, out_obj_ids, out_mask_logits in self.sam_model.propagate_in_video(self.inference_state, start_frame_idx=0):\n",
        "                masks = {\n",
        "                    out_obj_id: (out_mask_logits[i] > 0.0).squeeze().cpu().numpy()\n",
        "                    for i, out_obj_id in enumerate(out_obj_ids)\n",
        "                }\n",
        "                video_segments[out_frame_idx] = masks\n",
        "            # index_masks_all.append(self.make_index_mask(masks))\n",
        "\n",
        "        self.index_masks_all = [self.make_index_mask(v) for k, v in video_segments.items()]\n",
        "\n",
        "        out_frames, self.color_masks_all = colorize_masks(images, self.index_masks_all)\n",
        "        out_vidpath = \"tracked_colors.mp4\"\n",
        "        iio.mimwrite(out_vidpath, out_frames)\n",
        "        msg = f\"Wrote current tracked video to {out_vidpath}.\"\n",
        "        return out_vidpath, f\"{msg} Save the masks to an output directory if it looks good!\"\n",
        "\n",
        "    def save_masks_to_dir(self, output_dir: str) -> str:\n",
        "        \"\"\"Save color masks and index masks to a directory.\"\"\"\n",
        "        assert self.color_masks_all is not None\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        for img_path, clr_mask, idx_mask in zip(self.img_paths, self.color_masks_all, self.index_masks_all):\n",
        "            name = os.path.basename(img_path)\n",
        "            out_path = os.path.join(output_dir, name)\n",
        "            iio.imwrite(out_path, clr_mask)\n",
        "\n",
        "            base = Path(name).stem\n",
        "            np_out_path = os.path.join(output_dir, f\"{base}.npy\")\n",
        "            np.save(np_out_path, idx_mask)\n",
        "\n",
        "        message = f\"Saved masks to {output_dir}!\"\n",
        "        guru.debug(message)\n",
        "        return message"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYX7NDm5M_Uk"
      },
      "source": [
        "#### Other Utilities functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "s3XMslFKNCmq"
      },
      "outputs": [],
      "source": [
        "\n",
        "def is_image(file_path: str) -> bool:\n",
        "    return Path(file_path).suffix.lower() in (\".png\", \".jpg\", \".jpeg\")\n",
        "\n",
        "def draw_points(img, points, labels):\n",
        "    out = img.copy()\n",
        "    for p, label in zip(points, labels):\n",
        "        x, y = int(p[0]), int(p[1])\n",
        "        color = (0, 255, 0) if label == 1.0 else (255, 0, 0)\n",
        "        out = cv2.circle(out, (x, y), 10, color, -1)\n",
        "    return out\n",
        "\n",
        "\n",
        "def get_hls_palette(\n",
        "    n_colors: int,\n",
        "    lightness: float = 0.5,\n",
        "    saturation: float = 0.7,\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Generate a color palette in HLS space.\n",
        "\n",
        "    Args:\n",
        "        n_colors (int): Number of colors to generate\n",
        "        lightness (float): Lightness value for the HLS color space (0-1). Default is 0.5\n",
        "        saturation (float): Saturation value for the HLS color space (0-1). Default is 0.7\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Array of space (n_colors, 3) containing RGB colors (uint8).\n",
        "    \"\"\"\n",
        "    hues = np.linspace(0, 1, int(n_colors) + 1)[1:-1]  # (n_colors - 1)\n",
        "    # hues = (hues + first_hue) % 1\n",
        "    palette = [(0.0, 0.0, 0.0)] + [\n",
        "        colorsys.hls_to_rgb(h, lightness, saturation) for h in hues\n",
        "    ]\n",
        "    return (255 * np.asarray(palette)).astype(\"uint8\")\n",
        "\n",
        "\n",
        "def colorize_masks(images: list[np.ndarray], index_masks: list[np.ndarray], fac: float = 0.5):\n",
        "    \"\"\"\n",
        "    Apply color masks to a list of images based on indexed segmentation masks.\n",
        "\n",
        "    Args:\n",
        "        images (list[np.ndarray]): List of original image arrays (H X W X C).\n",
        "        index_masks (list[np.ndarray]): List of integer index masks (H X W X C).\n",
        "        flac (float): Blending factor for original image vs. color mask. Default is 0.5\n",
        "\n",
        "    Returns:\n",
        "        tuple[list[np.ndarray], list[np.ndarray]]:\n",
        "            - out_frames: List of blended image arrays (uint8).\n",
        "            - color_masks: List of RGB color mask arrays (uint8).\n",
        "    \"\"\"\n",
        "    max_idx = max([m.max() for m in index_masks])\n",
        "    guru.debug(f\"{max_idx=}\")\n",
        "    palette = get_hls_palette(max_idx + 1)\n",
        "\n",
        "    color_masks = []\n",
        "    out_frames = []\n",
        "    for img, mask in zip(images, index_masks):\n",
        "        clr_mask = palette[mask.astype(\"int\")]\n",
        "        color_masks.append(clr_mask)\n",
        "        out_u = compose_img_mask(img, clr_mask, fac)\n",
        "        out_frames.append(out_u)\n",
        "    return out_frames, color_masks\n",
        "\n",
        "\n",
        "def compose_img_mask(img: np.ndarray, color_mask: np.ndarray, fac: float = 0.5) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Blend an image with a color mask using a given factor.\n",
        "\n",
        "    Args:\n",
        "        img (np.ndarray): Original image array (H X W X C)\n",
        "        color_mask (np.ndarray): Color mask array (H X W X C)\n",
        "        fac (float): Blending factor for the original image. Default 0.5\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Blended image array, dtype uint8\n",
        "    \"\"\"\n",
        "    # Normalize to 0-1 blend, then convert back to uint8\n",
        "    out_f = fac * img / 255 + (1 - fac) * color_mask / 255\n",
        "    out_u = (255 * out_f).astype(\"uint8\")\n",
        "    return out_u\n",
        "\n",
        "\n",
        "def listdir(path: str) -> List[str]:\n",
        "    return sorted(os.listdir(path)) if path and os.path.isdir(path) else []\n",
        "\n",
        "def update_vid_root(root_dir: str, vid_dir_name: str) -> List[str]:\n",
        "    vid_root = os.path.join(root_dir, vid_dir_name)\n",
        "    vid_paths = listdir(vid_root)\n",
        "    guru.debug(f\"Updating video paths: {vid_paths=}\")\n",
        "    return vid_paths\n",
        "\n",
        "def update_img_root(root_dir: str, img_dir_name: str) -> Tuple[str, List[str]]:\n",
        "    img_root = os.path.join(root_dir, img_dir_name)\n",
        "    img_dirs = listdir(img_root)\n",
        "    guru.debug(f\"Updating img dirs: {img_dirs=}\")\n",
        "    return img_root, img_dirs\n",
        "\n",
        "def update_mask_dir(root_dir: str, mask_dir: str, seq_name: str) -> str:\n",
        "    return os.path.join(root_dir, mask_dir, seq_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "1NrHrowsNNcg"
      },
      "outputs": [],
      "source": [
        "START_INSTRUCTIONS = (\n",
        "    \"Select a video file to extract frames from, \"\n",
        "    \"or select an image directory with frames already extracted.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "V15eMTPoM7X1"
      },
      "outputs": [],
      "source": [
        "def make_demo(\n",
        "    checkpoint_dir: str,\n",
        "    model_cfg: str,\n",
        "    device: str,\n",
        "    root_dir: str,\n",
        "    vid_dir_name: str = \"videos\",\n",
        "    img_dir_name: str = \"images\",\n",
        "    mask_name: str = \"masks\",\n",
        ") -> gr.Blocks:\n",
        "    prompts = PromptGUI(checkpoint_dir, model_cfg, device)\n",
        "    vid_root = os.path.join(root_dir, vid_dir_name)\n",
        "    img_root = os.path.join(root_dir, img_dir_name)\n",
        "\n",
        "\n",
        "\n",
        "    with gr.Blocks() as demo:\n",
        "        instruction = gr.Textbox(START_INSTRUCTIONS, label=\"Instruction\", interactive=False)\n",
        "        with gr.Row():\n",
        "            root_dir_field = gr.Textbox(root_dir, label=\"Dataset root directory\", interactive=False)\n",
        "            vid_dir_field = gr.Textbox(vid_dir_name, label=\"Video subdirectory name\", interactive=False)\n",
        "            img_dir_field = gr.Textbox(img_dir_name, label=\"Image subdirectory name\", interactive=False)\n",
        "            mask_dir_field = gr.Textbox(mask_name, label=\"Mask subdirectory name\", interactive=False)\n",
        "            seq_name_field = gr.Textbox(\"\", label=\"Sequence name\", interactive=False)\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                vid_files = listdir(vid_root)\n",
        "                vid_files_field = gr.Dropdown(label=\"Video files\", choices=vid_files, value=None)\n",
        "                # vid_file_path = os.path.join(vid_root, vid_files_field.value)\n",
        "                # print(\"Selected videos path: \", vid_file_path)\n",
        "                input_video_field = gr.Video(label=\"Input Video\", autoplay=False, value=None, height=360)\n",
        "\n",
        "                with gr.Row():\n",
        "                    start_time = gr.Number(0, label=\"Start time (s)\")\n",
        "                    end_time = gr.Number(1, label=\"End time (s)\")\n",
        "                    sel_fps = gr.Number(30, label=\"FPS\")\n",
        "                    sel_height = gr.Number(540, label=\"Height\")\n",
        "                    extract_button = gr.Button(\"Extract frames\")\n",
        "\n",
        "            with gr.Column():\n",
        "                img_dirs = listdir(img_root)\n",
        "                print(img_dirs)\n",
        "                img_dirs_field = gr.Dropdown(label=\"Image directories\", choices=img_dirs, interactive=True, value=None)\n",
        "                input_img_dir_field = gr.Textbox(\"\", label=\"Input directory\", interactive=False)\n",
        "\n",
        "                print(\"-----------------------\")\n",
        "\n",
        "                frame_index_slider = gr.Slider(\n",
        "                    label=\"Frame index\",\n",
        "                    minimum=0,\n",
        "                    maximum=0, # max is updated later\n",
        "                    value=0,\n",
        "                    step=1,\n",
        "                )\n",
        "                sam_button = gr.Button(\"Get SAM features\")\n",
        "                reset_button = gr.Button(\"Reset\")\n",
        "                current_frame_image = gr.Image(label=\"Current Frame\", value=None)\n",
        "                with gr.Row():\n",
        "                    pos_button = gr.Button(\"Toggle positive\")\n",
        "                    neg_button = gr.Button(\"Toggle negative\")\n",
        "                clear_button = gr.Button(\"Clear points\")\n",
        "\n",
        "            with gr.Column():\n",
        "                output_img = gr.Image(label=\"Current selection\")\n",
        "                add_mask_button = gr.Button(\"Add New Mask\")\n",
        "                submit_mask_button = gr.Button(\"Submit Mask for Tracking\")\n",
        "                final_video_field = gr.Video(label=\"Masked Video\")\n",
        "                out_mask_path_field = gr.Textbox(None, label=\"Path to save masks\", interactive=False)\n",
        "                save_button = gr.Button(\"Save masks\")\n",
        "\n",
        "        def select_video_file(root_dir: str, vid_dir: str, video_file: str) -> Tuple[str, str]:\n",
        "            seq_name = os.path.splitext(video_file)[0]\n",
        "            guru.debug(f\"Selected video: {video_file=}\")\n",
        "            video_path = os.path.join(root_dir, vid_dir, video_file)\n",
        "            return seq_name, video_path\n",
        "\n",
        "        def extract_frames_from_video(\n",
        "            root_dir: str, vid_dir: str, img_dir: str, vid_file: str, start: float, end: float, fps: int, height: int, ext: str = \"jpeg\"\n",
        "        ):\n",
        "            # SAM2 supports the jpeg folder only\n",
        "            seq_name = os.path.splitext(vid_file)[0]\n",
        "            vid_path = os.path.join(root_dir, vid_dir, vid_file)\n",
        "            out_dir = os.path.join(root_dir, img_dir, seq_name)\n",
        "            guru.debug(f\"Extracting frames to {out_dir}\")\n",
        "            os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "            start_time_str = str(timedelta(seconds=start))\n",
        "            end_time_str = str(timedelta(seconds=end))\n",
        "            cmd = (\n",
        "                    f\"ffmpeg -ss {start_time_str} -to {end_time_str} -i {vid_path} \"\n",
        "                f\"-vf 'scale=-1:{int(height)},fps={int(fps)}' {out_dir}/%05d.{ext}\"\n",
        "            )\n",
        "            print(cmd)\n",
        "            subprocess.call(cmd, shell=True)\n",
        "            img_root = os.path.join(root_dir, img_dir)\n",
        "            img_dirs_updated = listdir(img_root)\n",
        "            print(\"INFOR:  Img dirs: \", img_dirs_updated)\n",
        "            # Update the img dir\n",
        "            num_imgs = prompts.set_img_dir(out_dir)\n",
        "            first_img_frame = prompts.set_input_image(0)\n",
        "            slider_update = gr.update(minimum=0, maximum=max(num_imgs - 1, 0), value=0)\n",
        "            msg = (\n",
        "                \"click the Get SAM Features and choose the frame you want to annotate\"\n",
        "            )\n",
        "            return out_dir, gr.update(choices=img_dirs_updated, value=seq_name), slider_update, first_img_frame, msg\n",
        "\n",
        "        def select_image_dir(root_dir: str, img_dir: str, seq_name: str):\n",
        "            img_dir_path = os.path.join(root_dir, img_dir, seq_name)\n",
        "            num_imgs = prompts.set_img_dir(img_dir_path)\n",
        "            slider_update = gr.update(minimum=0, maximum=max(num_imgs - 1, 0), value=0)\n",
        "            guru.debug(f\"Selected image dir: {img_dir}\")\n",
        "            input_image_update = prompts.set_input_image(0)\n",
        "            message = f\"Loaded {num_imgs} images from {img_dir_path}.\"\n",
        "            return slider_update, input_image_update, message\n",
        "\n",
        "        def on_frame_select(frame_idx: int) -> Optional[np.ndarray]:\n",
        "            img = prompts.set_input_image(frame_idx)\n",
        "            return img\n",
        "\n",
        "        def on_click_image(evt: gr.SelectData, frame_idx, img):\n",
        "            if evt.index is None or len(evt.index) < 2:\n",
        "                print(\"No valid coordinates selected\")\n",
        "                return img\n",
        "            j, i = evt.index  # Gradio provides [x, y]\n",
        "            print(f\"Selected coordinates: ({i}, {j}) on frame {frame_idx}\")\n",
        "            index_mask = prompts.add_point(frame_idx, i, j)\n",
        "            guru.debug(f\"{index_mask.shape=}\")\n",
        "            palette = get_hls_palette(index_mask.max() + 1)\n",
        "            color_mask = palette[index_mask]\n",
        "            out_u = compose_img_mask(img, color_mask)\n",
        "            out_img = draw_points(out_u, prompts.selected_points, prompts.selected_labels)\n",
        "            return out_img\n",
        "\n",
        "        # selecting a video file\n",
        "        vid_files_field.select(\n",
        "            select_video_file,\n",
        "            [root_dir_field, vid_dir_field, vid_files_field],\n",
        "            outputs=[seq_name_field, input_video_field],\n",
        "        )\n",
        "\n",
        "        # selecting an image directory\n",
        "        img_dirs_field.select(\n",
        "            select_image_dir,\n",
        "            [root_dir_field, img_dir_field, img_dirs_field],\n",
        "            [frame_index_slider, current_frame_image, input_img_dir_field],\n",
        "        )\n",
        "\n",
        "        # extracting frames from video\n",
        "        extract_button.click(\n",
        "            extract_frames_from_video,\n",
        "            [\n",
        "                root_dir_field,\n",
        "                vid_dir_field,\n",
        "                img_dir_field,\n",
        "                vid_files_field,\n",
        "                start_time,\n",
        "                end_time,\n",
        "                sel_fps,\n",
        "                sel_height,\n",
        "            ],\n",
        "            outputs=[input_img_dir_field, img_dirs_field, frame_index_slider, current_frame_image, instruction],\n",
        "        )\n",
        "\n",
        "        frame_index_slider.change(on_frame_select, [frame_index_slider], [current_frame_image])\n",
        "        current_frame_image.select(on_click_image, [frame_index_slider, current_frame_image], [output_img])\n",
        "\n",
        "        sam_button.click(prompts.get_sam_features, outputs=[instruction, current_frame_image])\n",
        "        reset_button.click(prompts.reset)\n",
        "        pos_button.click(prompts.set_positive, outputs=[instruction])\n",
        "        neg_button.click(prompts.set_negative, outputs=[instruction])\n",
        "        clear_button.click(prompts.clear_points, outputs=[output_img, final_video_field, instruction])\n",
        "\n",
        "        add_mask_button.click(prompts.add_new_mask, outputs=[output_img, instruction])\n",
        "        submit_mask_button.click(prompts.run_tracker, outputs=[final_video_field, instruction])\n",
        "\n",
        "        save_button.click(prompts.save_masks_to_dir, [out_mask_path_field], outputs=[instruction])\n",
        "\n",
        "    return demo"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Folder Structure\n",
        "\n",
        "```\n",
        "sam2/\n",
        " └── data/\n",
        "      ├── videos/   # Directory containing the videos to be tracked\n",
        "      │    ├── seq1.mp4\n",
        "      │    └── seq2.mp4\n",
        "      ├── images/   # Directory where the extracted frames are stored\n",
        "      └── masks/    # Directory where the tracked videos are saved\n",
        "```\n",
        "\n",
        "**Note:**\n",
        "Directly referencing the mounted Google Drive directory doesn’t seem to work properly. As a workaround, I copy the data from Drive to the Colab instance before running the main function.\n"
      ],
      "metadata": {
        "id": "VhGAKDjHhCEg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Yz2OypDQOrgI"
      },
      "outputs": [],
      "source": [
        "# Create the necessary directories if they don't exist\n",
        "os.makedirs('data/video', exist_ok=True)\n",
        "\n",
        "# Copy the video folder from Google Drive to the Colab environment\n",
        "!cp -r /content/drive/MyDrive/data/sam2-segmentation/video data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "uXIJBZzc3GEW"
      },
      "outputs": [],
      "source": [
        "DEFAULT_CHECKPOINT_DIR = \"checkpoints/sam2.1_hiera_large.pt\"\n",
        "# DEFAULT_MODEL_CFG = \"sam2.1_hiera_l.yaml\"\n",
        "DEFAULT_MODEL_CFG = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
        "# data where there data to be inferences\n",
        "DEFAULT_ROOT_DIR = \"data\"\n",
        "DEFAULT_VID_NAME = \"video\"\n",
        "DEFAULT_IMG_NAME = \"images\"\n",
        "DEFAULT_MASK_NAME = \"masks\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 908
        },
        "id": "dyYK-J-qbgUz",
        "outputId": "32c9f2e1-2df2-4dae-c1c2-12b4fbf4035c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2025-11-11 07:41:32.097\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_init_sam_model\u001b[0m:\u001b[36m39\u001b[0m - \u001b[1mLoaded model checkpoint from checkpoints/sam2.1_hiera_large.pt\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['7515858-sd_540_960_30fps']\n",
            "-----------------------\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://853faa724ada128e55.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://853faa724ada128e55.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2025-11-11 07:53:51.064\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mselect_video_file\u001b[0m:\u001b[36m73\u001b[0m - \u001b[34m\u001b[1mSelected video: video_file='7515858-sd_540_960_30fps.mp4'\u001b[0m\n",
            "\u001b[32m2025-11-11 07:53:58.350\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract_frames_from_video\u001b[0m:\u001b[36m84\u001b[0m - \u001b[34m\u001b[1mExtracting frames to data/images/7515858-sd_540_960_30fps\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ffmpeg -ss 0:00:00 -to 0:00:15 -i data/video/7515858-sd_540_960_30fps.mp4 -vf 'scale=-1:540,fps=30' data/images/7515858-sd_540_960_30fps/%05d.jpeg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2025-11-11 07:54:00.299\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mset_input_image\u001b[0m:\u001b[36m72\u001b[0m - \u001b[34m\u001b[1mSetting frame 0 / 437\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFOR:  Img dirs:  ['7515858-sd_540_960_30fps']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "frame loading (JPEG): 100%|██████████| 437/437 [00:15<00:00, 28.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected coordinates: (253, 117) on frame 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2025-11-11 07:55:29.248\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mon_click_image\u001b[0m:\u001b[36m127\u001b[0m - \u001b[34m\u001b[1mindex_mask.shape=(540, 304)\u001b[0m\n",
            "propagate in video: 100%|██████████| 437/437 [08:09<00:00,  1.12s/it]\n",
            "\u001b[32m2025-11-11 08:03:43.911\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcolorize_masks\u001b[0m:\u001b[36m52\u001b[0m - \u001b[34m\u001b[1mmax_idx=np.uint8(1)\u001b[0m\n",
            "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (304, 540) to (304, 544) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://853faa724ada128e55.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "demo = make_demo(\n",
        "    DEFAULT_CHECKPOINT_DIR,\n",
        "    DEFAULT_MODEL_CFG,\n",
        "    device,\n",
        "    DEFAULT_ROOT_DIR,\n",
        "    DEFAULT_VID_NAME,\n",
        "    DEFAULT_IMG_NAME,\n",
        "    DEFAULT_MASK_NAME\n",
        ")\n",
        "demo.launch(share=True, debug=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}